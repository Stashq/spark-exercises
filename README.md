# PySpark

## Launching

[Launching Applications with spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html)
*./bin/spark-submit --class \<main-class> --master \<master-url>  --deploy-mode \<deploy-mode> --conf \<key>=\<value> ... # other options \<application-jar> \[application-arguments]*

*./bin/pyspark --master local[x]* - Run pyspark on local machine with x processes ("*" means as many worker threads as logical cores on your machine).  
*./bin/pyspark --master yarn* - Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.  

## Practices

[Sales example](https://towardsdatascience.com/six-spark-exercises-to-rule-them-all-242445b24565)

pyspark-stubs - A collection of the Apache Spark stub files. These files were generated by stubgen and manually edited to include accurate type hints.  

## Programming

.writeStream.format('console').start().awaitTermination() - chain of commands made on streamed dataframe that:

- declare writing made transformations,
- declare what is format of writing data,
- start working in loop,
- force to stop terminate on program stop.

## Kafka

### Lounching

In kafka-folder/bin run following in separate terminals:

### Consumer

for message in consumer: ... - Kafka consumer works in never ending loop. Consumer will process only these message that was registered after consumer attachment.

### Topic retention policy

You can set retention over time and over memory size.

### Topic partitions

Topic can be divide into parititions. Thanks to this we are able to assign partitions to the machine in distributed system (not limiting topics).  
Using partitions we give up the order of events.

### Pyspark and Kafka

```python
df.selectExpr(
    "CAST(key AS STRING)", "CAST(value AS STRING)") \ # always use this line first  
    .select(from_json(col("value"), schema).alias('json')) \ # loading schema  
    .select('json.*') \ # exctracting columns which are part of 'json' column  
    .withColumn(
        'timestamp', to_timestamp(col('timestamp'),
        'yyyy-MM-dd HH:mm:ss')) # if you have datetime you can convert it  
```

[Windowing explained](https://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html)
window(20, 10) - windowing over 20 seconds with step of 10 seconds.
