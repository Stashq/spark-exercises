{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row, Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .config(\"spark.executor.memory\", \"3gb\") \\\n",
    "    .appName(\"Exercise1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = spark.read.csv(\"data/seller.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "sales = spark.read.csv(\"data/sales.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "products = spark.read.csv(\"data/products.csv\", header=True, mode=\"DROPMALFORMED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers, sales, products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many orders, how many products and how many sellers are in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"sellers\": sellers.count(), \"sales\": sales.count(), \"products\": products.count()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many products have been sold at least once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.select('product_id').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the product contained in more orders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.groupBy('product_id').count().filter(col('count') >= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.groupBy('date').agg(countDistinct('product_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.join(products, sales[\"product_id\"] == products[\"product_id\"], \"inner\").\\\n",
    "    agg(avg(products[\"price\"] * sales[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_res = sales.join(products, sales[\"product_id\"] == products[\"product_id\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_res.agg(avg(mid_res[\"price\"] * mid_res[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_res.select(avg(mid_res[\"price\"] * mid_res[\"num_pieces_sold\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sales.groupby(sales[\"product_id\"]).count().sort(col(\"count\").desc()).limit(100).collect()\n",
    "\n",
    "# Step 2 - What we want to do is:\n",
    "#  a. Duplicate the entries that we have in the dimension table for the most common products, e.g.\n",
    "#       product_0 will become: product_0-1, product_0-2, product_0-3 and so on\n",
    "#  b. On the sales table, we are going to replace \"product_0\" with a random duplicate (e.g. some of them \n",
    "#     will be replaced with product_0-1, others with product_0-2, etc.)\n",
    "# Using the new \"salted\" key will unskew the join\n",
    "\n",
    "# Let's create a dataset to do the trick\n",
    "REPLICATION_FACTOR = 101\n",
    "l = []\n",
    "replicated_products = []\n",
    "for _r in results:\n",
    "    replicated_products.append(_r[\"product_id\"])\n",
    "    for _rep in range(0, REPLICATION_FACTOR):\n",
    "        l.append((_r[\"product_id\"], _rep))\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "replicated_df = rdd.map(lambda x: Row(product_id=x[0], replication=int(x[1])))\n",
    "replicated_df = spark.createDataFrame(replicated_df)\n",
    "\n",
    "#   Step 3: Generate the salted key\n",
    "products = products.join(broadcast(replicated_df),\n",
    "                                     products[\"product_id\"] == replicated_df[\"product_id\"], \"left\"). \\\n",
    "    withColumn(\"salted_join_key\", when(replicated_df[\"replication\"].isNull(), products[\"product_id\"]).otherwise(\n",
    "    concat(replicated_df[\"product_id\"], lit(\"-\"), replicated_df[\"replication\"])))\n",
    "\n",
    "sales = sales.withColumn(\"salted_join_key\", when(sales[\"product_id\"].isin(replicated_products),\n",
    "                                                             concat(sales[\"product_id\"], lit(\"-\"),\n",
    "                                                                    round(rand() * (REPLICATION_FACTOR - 1), 0).cast(\n",
    "                                                                        IntegerType()))).otherwise(\n",
    "    sales[\"product_id\"]))\n",
    "\n",
    "#   Step 4: Finally let's do the join\n",
    "print(sales.join(products, sales[\"salted_join_key\"] == products[\"salted_join_key\"],\n",
    "                       \"inner\").\n",
    "      agg(avg(products[\"price\"] * sales[\"num_pieces_sold\"])).show())\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.join(broadcast(sellers), sales['seller_id'] == sellers['seller_id']).\\\n",
    "    groupBy(sales['seller_id']).agg(avg(sales[\"num_pieces_sold\"]/sellers['daily_target'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.join(broadcast(sellers), sales[\"seller_id\"] == sellers[\"seller_id\"], \"inner\").withColumn(\n",
    "    \"ratio\", sales[\"num_pieces_sold\"]/sellers[\"daily_target\"]\n",
    ").groupBy(sales[\"seller_id\"]).agg(avg(\"ratio\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Caching](https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/)  \n",
    "[row_number vs rand vs dense_rank](https://sparkbyexamples.com/pyspark/pyspark-window-functions/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = Window.partitionBy(\"product_id\").orderBy(col(\"n_sold\").asc())\n",
    "df1 = sales.groupBy(['seller_id', 'product_id']).agg(\n",
    "    sum('num_pieces_sold').alias('n_sold')) \\\n",
    "    .withColumn('seller_prod_rank', dense_rank().over(win))\n",
    "\n",
    "df2 = sales.groupBy('product_id').agg(countDistinct('seller_id').alias('n_sellers'))\n",
    "\n",
    "df = df1.join(df2, 'product_id')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one = df.filter(col('n_sellers') == 1).select(['product_id', 'seller_id', 'seller_prod_rank'])\n",
    "\n",
    "seconds = df.filter(col('seller_prod_rank') == 2).select(['product_id', 'seller_id', 'seller_prod_rank'])\n",
    "\n",
    "maxs = df.filter(col('seller_prod_rank') > 2) \\\n",
    "    .groupBy('product_id').agg(max('seller_prod_rank')).select('product_id')\n",
    "lasts = df.select(['product_id', 'seller_id', 'seller_prod_rank']) \\\n",
    "    .join(maxs, 'product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_one, seconds, lasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = only_one.union(seconds).union(lasts)\n",
    "result.persist()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.filter(col('product_id') == 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.withColumn(\n",
    "    'n_sellers_group', when(col('seller_prod_rank') == 1, 1) \\\n",
    "        .when(col('seller_prod_rank') == 2, 2) \\\n",
    "        .when(col('seller_prod_rank') > 2, 'more')) \\\n",
    "    .groupBy('n_sellers_group').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unpersist()\n",
    "result.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def custom_hash(order_id: str, bill: str):\n",
    "    if order_id % 2 == 0:\n",
    "        for _ in range(bill.count('A')):\n",
    "            bill = hashlib.md5(bill)\n",
    "    else:\n",
    "        bill = hashlib.sha256(bill)\n",
    "    return bill\n",
    "\n",
    "\n",
    "def even_hash(bill):\n",
    "    res = bill.encode(\"utf-8\")\n",
    "    for _ in range(bill.count('A')):\n",
    "        res = hashlib.md5(res).hexdigest().encode(\"utf-8\")\n",
    "    res = res.decode('utf-8')\n",
    "    return res\n",
    "\n",
    "\n",
    "even_hash_udf = spark.udf.register('even_hash', even_hash)\n",
    "\n",
    "\n",
    "result = sales.withColumn(\n",
    "    'hashed_bill',\n",
    "    when(col('order_id') % 2 == 0, even_hash_udf(col(\"bill_raw_text\"))) \\\n",
    "    .otherwise(sha2(col(\"bill_raw_text\"), 256))\n",
    ")\n",
    "\n",
    "result.show()\n",
    "result.groupBy('hashed_bill').count().filter(col('count') > 1).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d7cb144c8cc701a89d62b53283ed32d7685085060bd24dfceac0e5a96f99c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
